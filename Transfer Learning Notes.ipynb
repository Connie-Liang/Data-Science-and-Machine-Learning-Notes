{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array\n",
    "\n",
    "import os\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transfer Learning is really powerful and can be used for neural networks in terms of \"borrowing\" pretrained neural networks or weights to build off existing knowledge. \"Situation where what has been learned in one setting is exploited to improve generalization in another setting\". \n",
    "\n",
    "\n",
    "- For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks\n",
    "\n",
    "\n",
    "- \"After supervised learning, transfer learning will be the next driver of ML commercial success\" - Andrew Ng\n",
    "\n",
    "\n",
    "- \"the key motivation...is the fact that most models which solve complex problems need a whole lot of data, and getting vast amounts of labeled data for supervised models can be really difficult, considering the time and effort it takes to label data points.\" -Dipanjan Sarkar\n",
    "    \n",
    "    - transfer learning is particularly useful if you don't have a lot of data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for neural networks, TL will either mean:\n",
    "\n",
    "    - training your architecture and weights on one thing and then transferring those weights and architecture to your specific problem, or \n",
    "    \n",
    "    - using a pre-made architecture that has already been trained on a massive dataset (ex: ImageNet) to extract features and then use those features in another ML model training your data, or let the weights change for your specific data\n",
    "\n",
    "\n",
    "- for example, if you have a project that you're trying to classify accents on, but your primary dataset exhibits audio where the audio is not distinct enough, you may apply TL by first training your data on a smaller data set that is much more distinct in accent, and then using that trained neural network to train the primary network.\n",
    "\n",
    "\n",
    "- or, if you were trying to classify dog breeds using 50k images featuring 188 dog breeds. A CNN model might have only an accuracy of 3%. Then, using a pretrained architecture called Xception (designed for image classification, same architecture meaning the number of layers and convolutions) your accuracy jumps to 64%. Then, you fine tune by using pre-trained weights as well from ImageNet and including a very low learning rate, now your test accuracy jumps to 87%. \n",
    "\n",
    "This is the power of TL in taking data with tons of classifications and still being able to use black box models in a customized way to easily build an effective image classification machine learning model. Particularly in the category of image classification, there have been tremendous new breakthroughs done with TL. Exciting!! In terms of other branches of data science/machine learning, NLP/linguistics has had breakthroughs w GPT2 and GPT3. And so many of these black box models are open source and public!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Pre-trained Neural Network:\n",
    "\n",
    "- there are usually two ways:\n",
    "    - slicing and dicing the architecture layers of a pre-trained NN to then train your model on. For example, you might use only the first part of a NN architecture, and then building the second half yourself. Or you could take the output of the pre-trained NN, and incorporate them as features for your new model.\n",
    "    \n",
    "    - or...instead of messing with the actua structure of the NN architecture, to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
